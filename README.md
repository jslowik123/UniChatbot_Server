# Uni Chatbot API - Agent Edition

Ein moderner FastAPI-basierter Chatbot fÃ¼r UniversitÃ¤tsdokumente, der CrewAI-Agenten, pdfplumber und agentic RAG verwendet.

## ğŸš€ Features

- **CrewAI Agents**: Intelligente Agenten fÃ¼r sophistizierte Dokumentenanalyse
- **Agentic RAG**: Erweiterte Retrieval-Augmented Generation mit Multi-Query und Kontextkompression
- **PDFPlumber**: Verbesserte PDF-Extraktion fÃ¼r bessere TextqualitÃ¤t
- **Streaming Responses**: Echtzeit-Antworten fÃ¼r bessere Benutzererfahrung
- **Structured Outputs**: Strukturierte JSON-Antworten mit Metadaten, Quellen und Vertrauenswerten
- **Namespace-basierte Organisation**: Separate Dokumentensammlungen pro Namespace
- **Asynchrone Verarbeitung**: Celery-basierte Hintergrundverarbeitung
- **Firebase Integration**: Optional fÃ¼r Metadaten-Speicherung

## ğŸ—ï¸ Architektur

### Komponenten

1. **AgentProcessor** (`agent_processor.py`): Kernkomponente fÃ¼r Dokumentenverarbeitung und Agent-Setup
2. **AgentChatbot** (`agent_chatbot.py`): CrewAI-basierter Chatbot mit Streaming-UnterstÃ¼tzung
3. **FastAPI Main** (`main.py`): API-Endpunkte und Server-Konfiguration
4. **Celery Tasks** (`tasks.py`): Asynchrone Dokumentenverarbeitung
5. **Firebase Connection** (`firebase_connection.py`): Optional fÃ¼r Metadaten

### Technologien

- **FastAPI**: Web-Framework
- **CrewAI**: Agent-Framework
- **LangChain**: RAG-Pipeline
- **Pinecone**: Vektordatenbank
- **PDFPlumber**: PDF-Verarbeitung
- **Celery**: Asynchrone Tasks
- **Redis**: Message Broker
- **Firebase**: Metadaten-Speicherung (optional)

## ğŸ“‹ Anforderungen

- Python 3.8+
- Redis Server
- Pinecone API Key
- OpenAI API Key
- Firebase Credentials (optional)

## ğŸ› ï¸ Installation

1. **Dependencies installieren:**
```bash
pip install -r requirements.txt
```

2. **Environment Variables setzen:**
```bash
# .env Datei erstellen
PINECONE_API_KEY=your_pinecone_api_key
OPENAI_API_KEY=your_openai_api_key
REDIS_URL=redis://localhost:6379
# Optional fÃ¼r Firebase
FIREBASE_DATABASE_URL=your_firebase_url
FIREBASE_CREDENTIALS_PATH=path/to/credentials.json
```

3. **Redis Server starten:**
```bash
redis-server
```

4. **Celery Worker starten:**
```bash
celery -A celery_app worker --loglevel=info
```

5. **FastAPI Server starten:**
```bash
python main.py
```

## ğŸ”— API Endpunkte

### Grundlegende Endpunkte

- **GET `/`**: API-Informationen und Gesundheitsstatus
- **POST `/start_bot`**: Agent-Chatbot initialisieren

### Dokumentenmanagement

- **POST `/upload`**: PDF-Dokument hochladen und verarbeiten
  - `file`: PDF-Datei
  - `namespace`: Namespace fÃ¼r Organisation
  - `fileID`: Eindeutige Dokument-ID
  - `additionalInfo`: ZusÃ¤tzliche Informationen

- **POST `/delete`**: Dokument lÃ¶schen
  - `file_name`: Dateiname
  - `namespace`: Namespace
  - `fileID`: Dokument-ID
  - `just_firebase`: "true" fÃ¼r Pinecone-only LÃ¶schung

### Namespace-Management

- **POST `/create_namespace`**: Neuen Namespace erstellen
  - `namespace`: Name des Namespace
  - `dimension`: Vektordimension (default: 1536)

- **POST `/delete_namespace`**: Namespace lÃ¶schen
  - `namespace`: Name des Namespace

- **GET `/namespace_info/{namespace}`**: Namespace-Informationen abrufen

### Chat-Funktionen

- **POST `/send_message`**: Einfache Chat-Nachricht (nur Antwort-Text)
  - `user_input`: Benutzerfrage
  - `namespace`: Namespace fÃ¼r Dokumentensuche

- **POST `/send_message_structured`**: Strukturierte Chat-Nachricht mit Metadaten
  - `user_input`: Benutzerfrage
  - `namespace`: Namespace fÃ¼r Dokumentensuche

- **POST `/send_message_stream`**: Streaming-Chat mit Agent
  - `user_input`: Benutzerfrage
  - `namespace`: Namespace fÃ¼r Dokumentensuche

### Task-Management

- **GET `/task_status/{task_id}`**: Status einer asynchronen Aufgabe
- **GET `/test_worker`**: Celery Worker testen

## ğŸ”„ Strukturierte Ausgaben

Der Agent gibt jetzt strukturierte JSON-Antworten zurÃ¼ck:

```json
{
  "answer": "Die ausfÃ¼hrliche Antwort auf die Frage",
  "document_ids": ["doc1", "doc2", "doc3"],
  "sources": [
    "Originaltext aus Dokument 1, der die Antwort stÃ¼tzt",
    "Relevanter Text aus Dokument 2"
  ],
  "confidence_score": 0.9,
  "context_used": true,
  "additional_info": "ZusÃ¤tzliche Hinweise oder Informationen"
}
```

### Strukturierte Felder erklÃ¤rt:

- **`answer`**: Die hauptsÃ¤chliche Antwort auf die Frage
- **`document_ids`**: Liste der Dokument-IDs, die fÃ¼r die Antwort verwendet wurden
- **`sources`**: Originaltexte aus den Dokumenten, die die Antwort stÃ¼tzen
- **`confidence_score`**: Vertrauenswert der Antwort (0.0 - 1.0)
- **`context_used`**: Ob Chat-History-Kontext verwendet wurde
- **`additional_info`**: ZusÃ¤tzliche Informationen oder Hinweise

## ğŸ¤– Agent-Workflow

1. **Dokumenten-Upload**: PDF wird mit pdfplumber extrahiert
2. **Text-Segmentierung**: RecursiveCharacterTextSplitter erstellt Chunks
3. **Embedding-Generierung**: OpenAI text-embedding-ada-002
4. **Vektorspeicherung**: Pinecone mit Namespace-Organisation
5. **Agent-Setup**: CrewAI Agent mit Multi-Query Retriever
6. **Kontextkompression**: LLMChainExtractor fÃ¼r relevante Inhalte
7. **Strukturierte Antwortgenerierung**: GPT-4 mit agentic RAG und JSON-Output

## ğŸ“ Beispielverwendung

### Python Client (Strukturierte Antworten)

```python
import requests
import json

# Bot starten
response = requests.post("http://localhost:8000/start_bot")
print(response.json())

# Strukturierte Chat-Nachricht
data = {
    "user_input": "Was ist das Modul Software Engineering?",
    "namespace": "university"
}
response = requests.post("http://localhost:8000/send_message_structured", data=data)
result = response.json()

# Strukturierte Antwort verarbeiten
structured = result["structured_response"]
print("Antwort:", structured["answer"])
print("Vertrauen:", structured["confidence_score"])
print("Quellen:", structured["sources"])
print("Dokumente:", structured["document_ids"])
```

### Python Client (Einfache Antworten)

```python
# Einfache Chat-Nachricht (nur Text)
data = {
    "user_input": "Was ist das Modul Software Engineering?",
    "namespace": "university"
}
response = requests.post("http://localhost:8000/send_message", data=data)
result = response.json()
print(result["response"])  # Nur die Antwort
```

### JavaScript Client (Strukturiert)

```javascript
// Strukturierte Antwort
const formData = new FormData();
formData.append('user_input', 'Welche Module kann man wÃ¤hlen?');
formData.append('namespace', 'university');

fetch('/send_message_structured', {
    method: 'POST',
    body: formData
})
.then(response => response.json())
.then(data => {
    const structured = data.structured_response;
    console.log('Antwort:', structured.answer);
    console.log('Vertrauen:', structured.confidence_score);
    console.log('Quellen:', structured.sources);
    console.log('Chat-Kontext verwendet:', structured.context_used);
});
```

### JavaScript Client (Streaming mit Metadaten)

```javascript
// Streaming-Chat mit erweiterten Metadaten
const formData = new FormData();
formData.append('user_input', 'ErklÃ¤re mir die PrÃ¼fungsordnung');
formData.append('namespace', 'university');

const eventSource = new EventSource('/send_message_stream');
eventSource.onmessage = function(event) {
    const data = JSON.parse(event.data);
    if (data.type === 'chunk') {
        // Chunks kÃ¶nnen jetzt auch Emojis und Metadaten enthalten
        console.log(data.content);
    } else if (data.type === 'complete') {
        console.log('VollstÃ¤ndige Antwort:', data.fullResponse);
        eventSource.close();
    }
};
```

## ğŸ”§ Konfiguration

### Agent-Einstellungen

```python
# In agent_processor.py
EMBEDDING_MODEL = "text-embedding-ada-002"
DEFAULT_CHUNK_SIZE = 1000
DEFAULT_CHUNK_OVERLAP = 200
GPT_MODEL = "gpt-4"
```

### Strukturierte Ausgabe-Konfiguration

Der Agent wird explizit angewiesen, strukturierte JSON-Ausgaben zu produzieren:

```python
# Beispiel der strukturierten Task-Beschreibung
task_description = """
ğŸ”„ CHAT HISTORY BEACHTUNG - WICHTIG:
Du siehst eine Chat History mit vorherigen Nachrichten. BERÃœCKSICHTIGE diese aktiv:
- Beziehe dich auf vorherige Fragen und Antworten
- Nutze den Kontext aus frÃ¼heren Nachrichten
- Wenn der Nutzer "dazu", "darÃ¼ber", "das" oder Ã¤hnliche BezugswÃ¶rter verwendet, beziehe dich auf vorherige Themen

WICHTIG: Deine Antwort MUSS in folgendem JSON-Format sein:
{
    "answer": "Deine ausfÃ¼hrliche Antwort hier ohne AnfÃ¼hrungszeichen",
    "document_ids": ["doc1", "doc2"],
    "sources": ["Originaltext aus den Dokumenten"],
    "confidence_score": 0.9,
    "context_used": true,
    "additional_info": "ZusÃ¤tzliche Hinweise oder null"
}
"""
```

## ğŸ“Š Vertrauenswerte und QualitÃ¤t

### Confidence Score Interpretation:
- **ğŸŸ¢ 0.8-1.0**: Hohe VertrauenswÃ¼rdigkeit - Antwort basiert auf klaren, relevanten Quellen
- **ğŸŸ¡ 0.6-0.8**: Mittlere VertrauenswÃ¼rdigkeit - Antwort ist wahrscheinlich korrekt, aber weniger eindeutig
- **ğŸ”´ 0.0-0.6**: Niedrige VertrauenswÃ¼rdigkeit - Antwort kÃ¶nnte ungenau sein oder auf unzureichenden Informationen basieren

### QualitÃ¤tsindikatoren:
- **Anzahl Quellen**: Mehr Quellen = hÃ¶here VerlÃ¤sslichkeit
- **Chat-Kontext verwendet**: Zeigt an, ob die Antwort im Kontext der Unterhaltung steht
- **Dokument-IDs**: Identifiziert die genauen Dokumente, die fÃ¼r die Antwort verwendet wurden

## ğŸ”„ Migration von alter Version

Die neue Agent-Edition ersetzt folgende Komponenten:

- `doc_processor.py` â†’ `agent_processor.py`
- `chatbot.py` â†’ `agent_chatbot.py`
- `pinecone_connection.py` â†’ Direkte Integration in `agent_processor.py`
- PyPDF2 â†’ PDFPlumber
- Einfache RAG â†’ Agentic RAG mit CrewAI
- **Neu**: Strukturierte JSON-Ausgaben mit Metadaten

## ğŸ› Debugging

### Logs aktivieren

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Strukturierte Ausgaben debuggen

```python
# Die strukturierte Antwort analysieren
response = message_bot_structured("Test Frage", "namespace")
print(f"Confidence: {response['confidence_score']}")
print(f"Sources: {len(response['sources'])}")
print(f"Context used: {response['context_used']}")
```

### Celery Worker Debugging

```bash
celery -A celery_app worker --loglevel=debug
```

### Agent-Ausgaben aktivieren

```python
# In agent_processor.py
researcher = Agent(
    # ...
    verbose=True  # Bereits aktiviert
)
```

## ğŸ“ˆ Performance

- **Parallele Verarbeitung**: Celery fÃ¼r asynchrone Dokumentenverarbeitung
- **Effiziente Vektorsuche**: Pinecone mit Namespace-Isolation
- **Streaming**: Echtzeitantworten fÃ¼r bessere UX mit Metadaten
- **Kontextkompression**: Reduzierte Token-Nutzung
- **Chat-History**: Pro-Namespace Speicherung mit Context-Awareness
- **Strukturierte Ausgaben**: Optimiert fÃ¼r maschinelle Weiterverarbeitung

## ğŸ”’ Sicherheit

- **API-Key-Validierung**: Alle externen APIs erfordern gÃ¼ltige Keys
- **Namespace-Isolation**: Dokumente sind per Namespace getrennt
- **Input-Validierung**: Robuste Eingabevalidierung
- **Error-Handling**: Umfassendes Fehlerbehandlung mit strukturierten Fehlermeldungen
- **Vertrauenswerte**: Transparente QualitÃ¤tsbewertung der Antworten

## ğŸ“š Weitere Dokumentation

- [CrewAI Documentation](https://docs.crewai.com/)
- [LangChain Documentation](https://docs.langchain.com/)
- [Pinecone Documentation](https://docs.pinecone.io/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/) 