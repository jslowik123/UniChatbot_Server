# Kernprozesse des send_message Endpoints

## 1. Dokumentensuche und Kontext-Abruf

### Namespace-Daten abrufen
```python
database_overview = doc_processor.get_namespace_data(namespace)
```
- Extrahiert aus Firebase: Dokument-IDs, Namen, Keywords, Zusammenfassungen
- Struktur der Metadaten:
  ```json
  {
    "document_id": {
      "name": "Dokumentname",
      "keywords": ["keyword1", "keyword2"],
      "summary": "Zusammenfassung des Dokuments"
    }
  }
  ```

### Passendes Dokument finden
```python
appropriate_document = doc_processor.appropriate_document_search(
    namespace, database_overview, user_input
)
```
- KI-Analyse der Dokument-Metadaten mit GPT-4.1-nano
- System Prompt für Dokumentauswahl:
  ```python
  prompt_template = ChatPromptTemplate.from_messages([
      ("system", """Du bist ein präziser Dokumenten-Analyse-Agent. Deine Aufgabe ist es, das am besten passende Dokument für eine Benutzeranfrage zu finden.

      Verfügbare Dokumente:
      {database_overview}

      Benutzeranfrage:
      {user_input}

      Wichtige Regeln:
      - Analysiere die semantische Ähnlichkeit zwischen Anfrage und Dokument-Metadaten
      - Berücksichtige Keywords und Zusammenfassungen
      - Wähle das Dokument mit der höchsten Relevanz
      - Begründe deine Auswahl
      - Antworte im JSON-Format:
        {
          "id": "document_id",
          "relevance_score": 0.0-1.0,
          "reasoning": "Begründung"
        }""")
  ])
  ```
- Berücksichtigt:
  - Semantische Ähnlichkeit zwischen Anfrage und Keywords
  - Relevanz der Zusammenfassungen
  - Dokument-Kontext
- Gibt zurück:
  ```json
  {
    "id": "document_id",
    "relevance_score": 0.95,
    "reasoning": "Begründung für die Auswahl"
  }
  ```

### Vektor-Datenbank Abfrage
```python
results = con.query(
    query=user_input, 
    namespace=namespace, 
    fileID=appropriate_document["id"], 
    num_results=DEFAULT_NUM_RESULTS
)
```
- Prozess:
  1. Generiert Embedding für die Benutzeranfrage
  2. Sucht nach ähnlichen Textpassagen im Dokument
  3. Verwendet Cosine-Similarity für Relevanz-Scoring
- Parameter:
  - num_results: Anzahl der zurückgegebenen Textpassagen
  - namespace: Isoliert die Suche auf spezifischen Namespace
  - fileID: Begrenzt die Suche auf ein bestimmtes Dokument

### Kontext extrahieren
```python
context_parts = []
for match in results.matches:
    if hasattr(match, 'metadata') and 'text' in match.metadata:
        context_parts.append(match.metadata['text'])
context = "\n".join(context_parts)
```
- Extrahiert Text aus den gefundenen Ergebnissen
- Struktur der Matches:
  ```json
  {
    "matches": [
      {
        "id": "chunk_id",
        "score": 0.92,
        "metadata": {
          "text": "Auszug aus dem Dokument",
          "page": 1,
          "section": "Einleitung"
        }
      }
    ]
  }
  ```

## 2. Antwort-Generierung

### System Prompt
```python
prompt_template = ChatPromptTemplate.from_messages([
    ("system", """Du bist ein sachlicher, präziser und hilfreicher Assistenz-Chatbot für eine Universität. 
    Deine Antworten basieren ausschließlich auf zwei Quellen:  
    1. Allgemeines Wissen: {knowledge}  
    2. Hochschulspezifische Informationen: {context}  

    Dokumenten-ID: {document_id}  
    Datenbankübersicht: {database_overview}  

    Wichtige Regeln:
    - Antwortgrundlage: Stütze deine Antworten ausschließlich auf die bereitgestellten Quellen
    - Natürlicher Ton: Antworte natürlich und flüssig
    - Widersprüche: Weise auf Widersprüche hin
    - Fehlende Informationen: Teile fehlende Informationen mit
    - Antwortstil: Klar, professionell und verständlich
    - Rückfragen: Stelle Rückfragen bei Unklarheiten
    - Chat History: Beachte die Chat History
    - Struktur der Antwort: JSON-Format mit answer, document_id und source""")
])
```
- Verwendet ChatPromptTemplate für strukturierte Prompt-Generierung
- Kombiniert:
  - Allgemeines Wissen
  - Spezifische Dokumenteninformationen
  - Chat-Historie für Kontext

### Antwort-Format
```json
{
    "answer": "Die eigentliche Antwort auf die Frage",
    "document_id": "ID des verwendeten Dokuments",
    "source": "Der Originaltext aus dem Kontext"
}
```
- Strukturierte JSON-Antwort
- Enthält:
  - Generierte Antwort
  - Referenz zum Quell-Dokument
  - Originaltext für Transparenz

## 3. Notwendige Konfigurationen
### Firebase

### Pinecone
- PINECONE_API_KEY: API-Key für Pinecone Vector Database
- Index "userfiles":
  - Dimension: 1536 (für OpenAI Embeddings)
  - Metric: cosine
  - Pod Type: p1.x1 (oder höher für Produktion)

### OpenAI
- OPENAI_API_KEY: API-Key für OpenAI
- GPT-4.1-nano Modell:
  - Temperatur: 0.7
  - Max Tokens: 2000
  - Top P: 1
  - Frequency Penalty: 0
  - Presence Penalty: 0 